{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** \\\n",
    "**Email:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.Proposed Split Strategdy (split by person, not recordings):\n",
    "- Train: 70 people\n",
    "- Val: 15 people\n",
    "- Test: 15 people\n",
    "This makes sure the model generalizes to new speakers, not memorize voices.\n",
    "2. Kilian's Dataset\n",
    "- Split Kilian's data into 80% train, 10% validation, 10% test for speaker-specific fine tuning and evaluation\n",
    "training process:\n",
    "- Train on 70 people (general model)\n",
    "- Fine-tune on 7k Kilian recordings\n",
    "- 1.5k Kilian + original 15 people\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Part 1: 1-NN Classification\n",
    "positive = np.array([[1, 2], [1, 4], [5, 4]])\n",
    "negative = np.array([[3, 1], [3, 2]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.scatter(positive[:, 0], positive[:, 1], c='blue', s=150, marker='o', \n",
    "           label='Positive', edgecolors='black', linewidths=2)\n",
    "ax.scatter(negative[:, 0], negative[:, 1], c='red', s=150, marker='s', \n",
    "           label='Negative', edgecolors='black', linewidths=2)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(0, 6, 300), np.linspace(0, 5, 300))\n",
    "Z = np.zeros(xx.shape)\n",
    "\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        point = np.array([xx[i, j], yy[i, j]])\n",
    "        pos_dist = np.min(np.sqrt(np.sum((positive - point)**2, axis=1)))\n",
    "        neg_dist = np.min(np.sqrt(np.sum((negative - point)**2, axis=1)))\n",
    "        Z[i, j] = 1 if pos_dist < neg_dist else 0\n",
    "\n",
    "ax.contourf(xx, yy, Z, levels=1, colors=['lightcoral', 'lightblue'], alpha=0.3)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors=['black'], linewidths=2, linestyles='--')\n",
    "\n",
    "ax.set_xlabel('x₁', fontsize=12)\n",
    "ax.set_ylabel('x₂', fontsize=12)\n",
    "ax.set_title('1-NN Decision Boundary', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Part 2: Feature Scaling\n",
    "positive_2 = np.array([[100, 2], [100, 4], [500, 4]])\n",
    "negative_2 = np.array([[300, 1], [300, 2]])\n",
    "test_point = np.array([500, 1])\n",
    "\n",
    "print(\"Part 2: Feature Scaling\")\n",
    "print(f\"Test point: {test_point}\\n\")\n",
    "\n",
    "print(\"BEFORE SCALING:\")\n",
    "pos_dist = np.sqrt(np.sum((positive_2 - test_point)**2, axis=1))\n",
    "neg_dist = np.sqrt(np.sum((negative_2 - test_point)**2, axis=1))\n",
    "print(f\"Min distance to positive: {np.min(pos_dist):.2f}\")\n",
    "print(f\"Min distance to negative: {np.min(neg_dist):.2f}\")\n",
    "pred_before = \"POSITIVE\" if np.min(pos_dist) < np.min(neg_dist) else \"NEGATIVE\"\n",
    "print(f\"Prediction: {pred_before}\\n\")\n",
    "\n",
    "all_data = np.vstack([positive_2, negative_2, test_point.reshape(1, -1)])\n",
    "min_vals = all_data.min(axis=0)\n",
    "max_vals = all_data.max(axis=0)\n",
    "\n",
    "positive_scaled = (positive_2 - min_vals) / (max_vals - min_vals)\n",
    "negative_scaled = (negative_2 - min_vals) / (max_vals - min_vals)\n",
    "test_scaled = (test_point - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "print(\"AFTER SCALING:\")\n",
    "pos_dist_scaled = np.sqrt(np.sum((positive_scaled - test_scaled)**2, axis=1))\n",
    "neg_dist_scaled = np.sqrt(np.sum((negative_scaled - test_scaled)**2, axis=1))\n",
    "print(f\"Min distance to positive: {np.min(pos_dist_scaled):.3f}\")\n",
    "print(f\"Min distance to negative: {np.min(neg_dist_scaled):.3f}\")\n",
    "pred_after = \"POSITIVE\" if np.min(pos_dist_scaled) < np.min(neg_dist_scaled) else \"NEGATIVE\"\n",
    "print(f\"Prediction: {pred_after}\\n\")\n",
    "\n",
    "# Part 3: Handling Missing Values\n",
    "\"\"\"\n",
    "Method: Use only available features for distance calculation\n",
    "\n",
    "1. Compute distance using non-missing features only\n",
    "2. Normalize by sqrt(# of available features)\n",
    "3. Distance = sqrt(sum of squared diffs on available features)\n",
    "\n",
    "Example: If feature 2 is missing, only use features 1, 3, 4, ... for distance.\n",
    "\"\"\"\n",
    "# Part 4: High-dimensional Data\n",
    "\"\"\"\n",
    "Reasons:\n",
    "1. Manifold hypothesis: Real images lie on low-dimensional manifolds in high-D space\n",
    "2. Semantic similarity: Similar images have similar pixel patterns\n",
    "3. Often use learned features (CNN embeddings) instead of raw pixels\n",
    "4. Distance metrics still capture meaningful visual similarity\n",
    "\n",
    "need lots of data to maintain density\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. No. Computing w·x alone does not tell you the error. You need to compare the predictions sign(w·x) \n",
    "with the true labels y and calculate the fraction of mistakes.\n",
    "2. You don’t need to compute training error because the Perceptron guarantees zero training error if the data is linearly separable. \n",
    "At convergence, all training points are correctly classified.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([[10, -2], [12, 2]])\n",
    "y = np.array([1, -1])\n",
    "w = np.array([0, 0])\n",
    "b = 0\n",
    "learning_rate = 1\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(f\"  Positive: (10, -2), y = +1\")\n",
    "print(f\"  Negative: (12, 2), y = -1\")\n",
    "print(f\"Initial: w = {w}, b = {b}\")\n",
    "print(f\"Learning rate: {learning_rate}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PERCEPTRON UPDATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "updates = []\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    mistakes = 0\n",
    "    print(f\"\\nIteration {iteration}:\")\n",
    "    print(f\"  Current: w = {w}, b = {b}\")\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        \n",
    "        score = np.dot(w, x_i) + b\n",
    "        y_pred = np.sign(score) if score != 0 else 1\n",
    "        \n",
    "        print(f\"\\n  Point {i+1}: x = {x_i}, y = {y_i}\")\n",
    "        print(f\"    Score: w·x + b = {w}·{x_i} + {b} = {score}\")\n",
    "        print(f\"    Prediction: {int(y_pred)}\")\n",
    "        \n",
    "        if y_i * score <= 0:  \n",
    "            mistakes += 1\n",
    "            print(f\"    MISTAKE! Updating...\")\n",
    "            \n",
    "            w_old = w.copy()\n",
    "            b_old = b\n",
    "            \n",
    "            w = w + learning_rate * y_i * x_i\n",
    "            b = b + learning_rate * y_i\n",
    "            \n",
    "            print(f\"    w: {w_old} + {learning_rate}*{y_i}*{x_i} = {w}\")\n",
    "            print(f\"    b: {b_old} + {learning_rate}*{y_i} = {b}\")\n",
    "            \n",
    "            updates.append({\n",
    "                'iteration': iteration,\n",
    "                'point': i+1,\n",
    "                'w': w.copy(),\n",
    "                'b': b\n",
    "            })\n",
    "        else:\n",
    "            print(f\"    Correct!\")\n",
    "    \n",
    "    iteration += 1\n",
    "    \n",
    "    if mistakes == 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"CONVERGED after {iteration} iterations!\")\n",
    "        print(f\"Final: w = {w}, b = {b}\")\n",
    "        print(f\"Total updates: {len(updates)}\")\n",
    "        break\n",
    "    \n",
    "    if iteration > 100:\n",
    "        print(\"Max iterations reached!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SEQUENCE OF WEIGHT VECTORS:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"w_0 = [0, 0], b_0 = 0 (initial)\")\n",
    "for i, update in enumerate(updates):\n",
    "    print(f\"w_{i+1} = {update['w']}, b_{i+1} = {update['b']} \"\n",
    "          f\"(after point {update['point']} in iteration {update['iteration']})\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(updates)+1, figsize=(5*(len(updates)+1), 4))\n",
    "if len(updates) == 0:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx in range(len(updates)+1):\n",
    "    ax = axes[idx] if len(updates) > 0 else axes[0]\n",
    "    \n",
    "    if idx == 0:\n",
    "        w_plot = np.array([0, 0])\n",
    "        b_plot = 0\n",
    "        title = \"Initial\"\n",
    "    else:\n",
    "        w_plot = updates[idx-1]['w']\n",
    "        b_plot = updates[idx-1]['b']\n",
    "        title = f\"After Update {idx}\"\n",
    "    \n",
    "    ax.scatter(X[y==1, 0], X[y==1, 1], c='blue', s=200, marker='o',\n",
    "              label='Positive (+1)', edgecolors='black', linewidths=2)\n",
    "    ax.scatter(X[y==-1, 0], X[y==-1, 1], c='red', s=200, marker='s',\n",
    "              label='Negative (-1)', edgecolors='black', linewidths=2)\n",
    "    \n",
    "    if not np.allclose(w_plot, 0):\n",
    "        x_vals = np.array([8, 14])\n",
    "        if w_plot[1] != 0:\n",
    "            y_vals = -(w_plot[0]*x_vals + b_plot)/w_plot[1]\n",
    "            ax.plot(x_vals, y_vals, 'g-', linewidth=2, label='Decision Boundary')\n",
    "        else:\n",
    "            x_boundary = -b_plot/w_plot[0]\n",
    "            ax.axvline(x_boundary, color='g', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    ax.set_xlabel('x₁', fontsize=11)\n",
    "    ax.set_ylabel('x₂', fontsize=11)\n",
    "    ax.set_title(f'{title}\\nw={w_plot}, b={b_plot}', fontsize=10, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(8, 14)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('perceptron_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "updates_log = [\n",
    "    {'x': np.array([0, 0, 0, 0, 4]), 'y': 1, 'count': 2},\n",
    "    {'x': np.array([0, 0, 6, 5, 0]), 'y': 1, 'count': 1},\n",
    "    {'x': np.array([3, 0, 0, 0, 0]), 'y': -1, 'count': 1},\n",
    "    {'x': np.array([0, 9, 3, 6, 0]), 'y': -1, 'count': 1},\n",
    "    {'x': np.array([0, 1, 0, 2, 5]), 'y': -1, 'count': 1},\n",
    "]\n",
    "\n",
    "print(\"Update log:\")\n",
    "print(\"x                    y   count\")\n",
    "print(\"-\" * 40)\n",
    "for update in updates_log:\n",
    "    print(f\"{str(update['x']):20s} {update['y']:2d}  {update['count']}\")\n",
    "\n",
    "print(f\"\\nInitial weight: w_0 = [0, 0, 0, 0, 0]\")\n",
    "print(f\"Learning rate: α = 1\")\n",
    "print(f\"\\nUpdate rule: w ← w + α * y * x\")\n",
    "\n",
    "w = np.array([0, 0, 0, 0, 0])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPUTING FINAL WEIGHT VECTOR:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "step = 0\n",
    "for update in updates_log:\n",
    "    x = update['x']\n",
    "    y = update['y']\n",
    "    count = update['count']\n",
    "    \n",
    "    for _ in range(count):\n",
    "        w_old = w.copy()\n",
    "        w = w + y * x\n",
    "        step += 1\n",
    "        print(f\"\\nStep {step}: w ← w + ({y}) * {x}\")\n",
    "        print(f\"         {w_old} + {y * x}\")\n",
    "        print(f\"       = {w}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL WEIGHT VECTOR: w = {w}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  - Started with 5-dimensional zero vector\")\n",
    "print(f\"  - Applied {step} updates total\")\n",
    "print(f\"  - Final weight vector has {len(w)} dimensions\")\n",
    "print(f\"  - Components: w = [{w[0]}, {w[1]}, {w[2]}, {w[3]}, {w[4]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "n_positive = 15\n",
    "n_negative = 15\n",
    "positive_data = np.random.randn(n_positive, 2) * 0.5 + np.array([2, 3])\n",
    "negative_data = np.random.randn(n_negative, 2) * 0.5 + np.array([5, 1])\n",
    "X_train = np.vstack([positive_data, negative_data])\n",
    "y_train = np.array([1]*n_positive + [-1]*n_negative)\n",
    "print(f\"Generated dataset:\")\n",
    "print(f\"  - {n_positive} positive examples\")\n",
    "print(f\"  - {n_negative} negative examples\")\n",
    "print(f\"  - 2D features (linearly separable)\")\n",
    "\n",
    "def perceptron_train_visualize(X, y, learning_rate=1.0, max_iter=100):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    history = [{'w': w.copy(), 'b': b, 'mistakes': None}]\n",
    "    for iteration in range(max_iter):\n",
    "        mistakes = 0    \n",
    "        for i in range(n_samples):\n",
    "            x_i = X[i]\n",
    "            y_i = y[i]    \n",
    "            if y_i * (np.dot(w, x_i) + b) <= 0:\n",
    "                w = w + learning_rate * y_i * x_i\n",
    "                b = b + learning_rate * y_i\n",
    "                mistakes += 1 \n",
    "                history.append({'w': w.copy(), 'b': b, 'mistakes': mistakes})\n",
    "        if mistakes == 0:\n",
    "            print(f\"\\nConverged after {iteration + 1} iterations\")\n",
    "            print(f\"Total updates: {len(history) - 1}\")\n",
    "            break\n",
    "    \n",
    "    return w, b, history\n",
    "w_final, b_final, history = perceptron_train_visualize(X_train, y_train)\n",
    "print(f\"Final weights: w = {w_final}, b = {b_final}\")\n",
    "key_steps = [0, len(history)//4, len(history)//2, 3*len(history)//4, -1]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, step in enumerate(key_steps):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    w_plot = history[step]['w']\n",
    "    b_plot = history[step]['b']\n",
    "    ax.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1],\n",
    "              c='blue', s=100, alpha=0.6, marker='o', label='Positive',\n",
    "              edgecolors='black', linewidths=1.5)\n",
    "    ax.scatter(X_train[y_train==-1, 0], X_train[y_train==-1, 1],\n",
    "              c='red', s=100, alpha=0.6, marker='s', label='Negative',\n",
    "              edgecolors='black', linewidths=1.5)\n",
    "    \n",
    "    if not np.allclose(w_plot, 0):\n",
    "        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "        xx = np.linspace(x_min, x_max, 100)\n",
    "        if abs(w_plot[1]) > 1e-5:\n",
    "            yy = -(w_plot[0] * xx + b_plot) / w_plot[1]\n",
    "            ax.plot(xx, yy, 'g-', linewidth=2.5, label='Decision Boundary')\n",
    "            \n",
    "            yy_fill = np.linspace(y_min, y_max, 100)\n",
    "            xx_fill = np.linspace(x_min, x_max, 100)\n",
    "            XX, YY = np.meshgrid(xx_fill, yy_fill)\n",
    "            Z = w_plot[0] * XX + w_plot[1] * YY + b_plot\n",
    "            ax.contourf(XX, YY, Z, levels=[-1000, 0, 1000],\n",
    "                       colors=['mistyrose', 'lightblue'], alpha=0.3)\n",
    "        \n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    if step == 0:\n",
    "        title = f\"Initial (Step 0)\"\n",
    "    elif step == -1:\n",
    "        title = f\"Final (Step {len(history)-1})\\n✓ CONVERGED\"\n",
    "    else:\n",
    "        title = f\"Step {step}\"\n",
    "    \n",
    "    ax.set_xlabel('Feature 1', fontsize=10)\n",
    "    ax.set_ylabel('Feature 2', fontsize=10)\n",
    "    ax.set_title(f'{title}\\nw=[{w_plot[0]:.2f}, {w_plot[1]:.2f}], b={b_plot:.2f}',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "if len(key_steps) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Perceptron Learning: Decision Boundary Evolution',\n",
    "            fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('perceptron_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
